{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"0b9ffbb2-8d33-eb0c-5a42-0733bdc3b74b"},"source":"**Introduction**\n\nThis kernel is an attempt to use every trick in the books to unleash the full power of Linear Regression, including a lot of preprocessing and a look at several Regularization algorithms.\n\nAt the time of writing, it achieves a score of about 0.121 on the public LB, just using regression, no RF, no xgboost, no ensembling etc. All comments/corrections are more than welcome."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2ba3154a-c2aa-3158-1984-63ad2c0c786a"},"outputs":[],"source":"# Imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import skew\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Definitions\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n%matplotlib inline\n#njobs = 4"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21fa35be-878b-b4f2-ef6e-68dc070b8bfa"},"outputs":[],"source":"# Get data\ntrain = pd.read_csv(\"../input/train.csv\")\nprint(\"train : \" + str(train.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7133b089-cd03-a56e-e8a4-8928b28a3c99"},"outputs":[],"source":"# Check for duplicates\nidsUnique = len(set(train.Id))\nidsTotal = train.shape[0]\nidsDupli = idsTotal - idsUnique\nprint(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")\n\n# Drop Id column\ntrain.drop(\"Id\", axis = 1, inplace = True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"be11c1c7-acfe-cb9a-bc81-95d461b884c5"},"source":"**Preprocessing**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9e99565-3296-cb48-3cc7-722b94e3fdac"},"outputs":[],"source":"# Looking for outliers, as indicated in https://ww2.amstat.org/publications/jse/v19n3/decock.pdf\nplt.scatter(train.GrLivArea, train.SalePrice, c = \"blue\", marker = \"s\")\nplt.title(\"Looking for outliers\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\ntrain = train[train.GrLivArea < 4000]"},{"cell_type":"markdown","metadata":{"_cell_guid":"2ff6d8cc-1f4b-776c-43d7-e9061eb5bed3"},"source":"There seems to be 2 extreme outliers on the bottom right, really large houses that sold for really cheap. More generally, the author of the dataset recommends removing 'any houses with more than 4000 square feet' from the dataset.  \nReference : https://ww2.amstat.org/publications/jse/v19n3/decock.pdf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"523ad216-4972-bf16-9328-722c461d1e98"},"outputs":[],"source":"# Log transform the target for official scoring\ntrain.SalePrice = np.log1p(train.SalePrice)\ny = train.SalePrice"},{"cell_type":"markdown","metadata":{"_cell_guid":"9b1a5238-976a-e7ab-00d1-0e92f156e407"},"source":"Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1fecd55e-0392-4d08-c4f7-da4326f42538"},"outputs":[],"source":"# Handle missing values for features where median/mean or most common value doesn't make sense\n\n# Alley : data description says NA means \"no alley access\"\ntrain.loc[:, \"Alley\"] = train.loc[:, \"Alley\"].fillna(\"None\")\n# BedroomAbvGr : NA most likely means 0\ntrain.loc[:, \"BedroomAbvGr\"] = train.loc[:, \"BedroomAbvGr\"].fillna(0)\n# BsmtQual etc : data description says NA for basement features is \"no basement\"\ntrain.loc[:, \"BsmtQual\"] = train.loc[:, \"BsmtQual\"].fillna(\"No\")\ntrain.loc[:, \"BsmtCond\"] = train.loc[:, \"BsmtCond\"].fillna(\"No\")\ntrain.loc[:, \"BsmtExposure\"] = train.loc[:, \"BsmtExposure\"].fillna(\"No\")\ntrain.loc[:, \"BsmtFinType1\"] = train.loc[:, \"BsmtFinType1\"].fillna(\"No\")\ntrain.loc[:, \"BsmtFinType2\"] = train.loc[:, \"BsmtFinType2\"].fillna(\"No\")\ntrain.loc[:, \"BsmtFullBath\"] = train.loc[:, \"BsmtFullBath\"].fillna(0)\ntrain.loc[:, \"BsmtHalfBath\"] = train.loc[:, \"BsmtHalfBath\"].fillna(0)\ntrain.loc[:, \"BsmtUnfSF\"] = train.loc[:, \"BsmtUnfSF\"].fillna(0)\n# CentralAir : NA most likely means No\ntrain.loc[:, \"CentralAir\"] = train.loc[:, \"CentralAir\"].fillna(\"N\")\n# Condition : NA most likely means Normal\ntrain.loc[:, \"Condition1\"] = train.loc[:, \"Condition1\"].fillna(\"Norm\")\ntrain.loc[:, \"Condition2\"] = train.loc[:, \"Condition2\"].fillna(\"Norm\")\n# EnclosedPorch : NA most likely means no enclosed porch\ntrain.loc[:, \"EnclosedPorch\"] = train.loc[:, \"EnclosedPorch\"].fillna(0)\n# External stuff : NA most likely means average\ntrain.loc[:, \"ExterCond\"] = train.loc[:, \"ExterCond\"].fillna(\"TA\")\ntrain.loc[:, \"ExterQual\"] = train.loc[:, \"ExterQual\"].fillna(\"TA\")\n# Fence : data description says NA means \"no fence\"\ntrain.loc[:, \"Fence\"] = train.loc[:, \"Fence\"].fillna(\"No\")\n# FireplaceQu : data description says NA means \"no fireplace\"\ntrain.loc[:, \"FireplaceQu\"] = train.loc[:, \"FireplaceQu\"].fillna(\"No\")\ntrain.loc[:, \"Fireplaces\"] = train.loc[:, \"Fireplaces\"].fillna(0)\n# Functional : data description says NA means typical\ntrain.loc[:, \"Functional\"] = train.loc[:, \"Functional\"].fillna(\"Typ\")\n# GarageType etc : data description says NA for garage features is \"no garage\"\ntrain.loc[:, \"GarageType\"] = train.loc[:, \"GarageType\"].fillna(\"No\")\ntrain.loc[:, \"GarageFinish\"] = train.loc[:, \"GarageFinish\"].fillna(\"No\")\ntrain.loc[:, \"GarageQual\"] = train.loc[:, \"GarageQual\"].fillna(\"No\")\ntrain.loc[:, \"GarageCond\"] = train.loc[:, \"GarageCond\"].fillna(\"No\")\ntrain.loc[:, \"GarageArea\"] = train.loc[:, \"GarageArea\"].fillna(0)\ntrain.loc[:, \"GarageCars\"] = train.loc[:, \"GarageCars\"].fillna(0)\n# HalfBath : NA most likely means no half baths above grade\ntrain.loc[:, \"HalfBath\"] = train.loc[:, \"HalfBath\"].fillna(0)\n# HeatingQC : NA most likely means typical\ntrain.loc[:, \"HeatingQC\"] = train.loc[:, \"HeatingQC\"].fillna(\"TA\")\n# KitchenAbvGr : NA most likely means 0\ntrain.loc[:, \"KitchenAbvGr\"] = train.loc[:, \"KitchenAbvGr\"].fillna(0)\n# KitchenQual : NA most likely means typical\ntrain.loc[:, \"KitchenQual\"] = train.loc[:, \"KitchenQual\"].fillna(\"TA\")\n# LotFrontage : NA most likely means no lot frontage\ntrain.loc[:, \"LotFrontage\"] = train.loc[:, \"LotFrontage\"].fillna(0)\n# LotShape : NA most likely means regular\ntrain.loc[:, \"LotShape\"] = train.loc[:, \"LotShape\"].fillna(\"Reg\")\n# MasVnrType : NA most likely means no veneer\ntrain.loc[:, \"MasVnrType\"] = train.loc[:, \"MasVnrType\"].fillna(\"None\")\ntrain.loc[:, \"MasVnrArea\"] = train.loc[:, \"MasVnrArea\"].fillna(0)\n# MiscFeature : data description says NA means \"no misc feature\"\ntrain.loc[:, \"MiscFeature\"] = train.loc[:, \"MiscFeature\"].fillna(\"No\")\ntrain.loc[:, \"MiscVal\"] = train.loc[:, \"MiscVal\"].fillna(0)\n# OpenPorchSF : NA most likely means no open porch\ntrain.loc[:, \"OpenPorchSF\"] = train.loc[:, \"OpenPorchSF\"].fillna(0)\n# PavedDrive : NA most likely means not paved\ntrain.loc[:, \"PavedDrive\"] = train.loc[:, \"PavedDrive\"].fillna(\"N\")\n# PoolQC : data description says NA means \"no pool\"\ntrain.loc[:, \"PoolQC\"] = train.loc[:, \"PoolQC\"].fillna(\"No\")\ntrain.loc[:, \"PoolArea\"] = train.loc[:, \"PoolArea\"].fillna(0)\n# SaleCondition : NA most likely means normal sale\ntrain.loc[:, \"SaleCondition\"] = train.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n# ScreenPorch : NA most likely means no screen porch\ntrain.loc[:, \"ScreenPorch\"] = train.loc[:, \"ScreenPorch\"].fillna(0)\n# TotRmsAbvGrd : NA most likely means 0\ntrain.loc[:, \"TotRmsAbvGrd\"] = train.loc[:, \"TotRmsAbvGrd\"].fillna(0)\n# Utilities : NA most likely means all public utilities\ntrain.loc[:, \"Utilities\"] = train.loc[:, \"Utilities\"].fillna(\"AllPub\")\n# WoodDeckSF : NA most likely means no wood deck\ntrain.loc[:, \"WoodDeckSF\"] = train.loc[:, \"WoodDeckSF\"].fillna(0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9de2ccc-4a07-a8f2-75d8-5151ff653e86"},"outputs":[],"source":"# Some numerical features are actually really categories\ntrain = train.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n                      })"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c57fec5-4eb9-9130-cf96-54e00275a594"},"outputs":[],"source":"# Encode some categorical features as ordered numbers when there is information in the order\ntrain = train.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )"},{"cell_type":"markdown","metadata":{"_cell_guid":"5fedb3e5-d6e0-efdd-d92a-8079377f82a9"},"source":"Then we will create new features, in 3 ways : \n\n 1. Simplifications of existing features\n 2. Combinations of existing features\n 3. Polynomials on the top 10 existing features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10327932-cac0-9310-1041-4572e72b44eb"},"outputs":[],"source":"# Create new features\n# 1* Simplifications of existing features\ntrain[\"SimplOverallQual\"] = train.OverallQual.replace({1 : 1, 2 : 1, 3 : 1, # bad\n                                                       4 : 2, 5 : 2, 6 : 2, # average\n                                                       7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                                      })\ntrain[\"SimplOverallCond\"] = train.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, # bad\n                                                       4 : 2, 5 : 2, 6 : 2, # average\n                                                       7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                                      })\ntrain[\"SimplPoolQC\"] = train.PoolQC.replace({1 : 1, 2 : 1, # average\n                                             3 : 2, 4 : 2 # good\n                                            })\ntrain[\"SimplGarageCond\"] = train.GarageCond.replace({1 : 1, # bad\n                                                     2 : 1, 3 : 1, # average\n                                                     4 : 2, 5 : 2 # good\n                                                    })\ntrain[\"SimplGarageQual\"] = train.GarageQual.replace({1 : 1, # bad\n                                                     2 : 1, 3 : 1, # average\n                                                     4 : 2, 5 : 2 # good\n                                                    })\ntrain[\"SimplFireplaceQu\"] = train.FireplaceQu.replace({1 : 1, # bad\n                                                       2 : 1, 3 : 1, # average\n                                                       4 : 2, 5 : 2 # good\n                                                      })\ntrain[\"SimplFireplaceQu\"] = train.FireplaceQu.replace({1 : 1, # bad\n                                                       2 : 1, 3 : 1, # average\n                                                       4 : 2, 5 : 2 # good\n                                                      })\ntrain[\"SimplFunctional\"] = train.Functional.replace({1 : 1, 2 : 1, # bad\n                                                     3 : 2, 4 : 2, # major\n                                                     5 : 3, 6 : 3, 7 : 3, # minor\n                                                     8 : 4 # typical\n                                                    })\ntrain[\"SimplKitchenQual\"] = train.KitchenQual.replace({1 : 1, # bad\n                                                       2 : 1, 3 : 1, # average\n                                                       4 : 2, 5 : 2 # good\n                                                      })\ntrain[\"SimplHeatingQC\"] = train.HeatingQC.replace({1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  })\ntrain[\"SimplBsmtFinType1\"] = train.BsmtFinType1.replace({1 : 1, # unfinished\n                                                         2 : 1, 3 : 1, # rec room\n                                                         4 : 2, 5 : 2, 6 : 2 # living quarters\n                                                        })\ntrain[\"SimplBsmtFinType2\"] = train.BsmtFinType2.replace({1 : 1, # unfinished\n                                                         2 : 1, 3 : 1, # rec room\n                                                         4 : 2, 5 : 2, 6 : 2 # living quarters\n                                                        })\ntrain[\"SimplBsmtCond\"] = train.BsmtCond.replace({1 : 1, # bad\n                                                 2 : 1, 3 : 1, # average\n                                                 4 : 2, 5 : 2 # good\n                                                })\ntrain[\"SimplBsmtQual\"] = train.BsmtQual.replace({1 : 1, # bad\n                                                 2 : 1, 3 : 1, # average\n                                                 4 : 2, 5 : 2 # good\n                                                })\ntrain[\"SimplExterCond\"] = train.ExterCond.replace({1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  })\ntrain[\"SimplExterQual\"] = train.ExterQual.replace({1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  })\n\n# 2* Combinations of existing features\n# Overall quality of the house\ntrain[\"OverallGrade\"] = train[\"OverallQual\"] * train[\"OverallCond\"]\n# Overall quality of the garage\ntrain[\"GarageGrade\"] = train[\"GarageQual\"] * train[\"GarageCond\"]\n# Overall quality of the exterior\ntrain[\"ExterGrade\"] = train[\"ExterQual\"] * train[\"ExterCond\"]\n# Overall kitchen score\ntrain[\"KitchenScore\"] = train[\"KitchenAbvGr\"] * train[\"KitchenQual\"]\n# Overall fireplace score\ntrain[\"FireplaceScore\"] = train[\"Fireplaces\"] * train[\"FireplaceQu\"]\n# Overall garage score\ntrain[\"GarageScore\"] = train[\"GarageArea\"] * train[\"GarageQual\"]\n# Overall pool score\ntrain[\"PoolScore\"] = train[\"PoolArea\"] * train[\"PoolQC\"]\n# Simplified overall quality of the house\ntrain[\"SimplOverallGrade\"] = train[\"SimplOverallQual\"] * train[\"SimplOverallCond\"]\n# Simplified overall quality of the exterior\ntrain[\"SimplExterGrade\"] = train[\"SimplExterQual\"] * train[\"SimplExterCond\"]\n# Simplified overall pool score\ntrain[\"SimplPoolScore\"] = train[\"PoolArea\"] * train[\"SimplPoolQC\"]\n# Simplified overall garage score\ntrain[\"SimplGarageScore\"] = train[\"GarageArea\"] * train[\"SimplGarageQual\"]\n# Simplified overall fireplace score\ntrain[\"SimplFireplaceScore\"] = train[\"Fireplaces\"] * train[\"SimplFireplaceQu\"]\n# Simplified overall kitchen score\ntrain[\"SimplKitchenScore\"] = train[\"KitchenAbvGr\"] * train[\"SimplKitchenQual\"]\n# Total number of bathrooms\ntrain[\"TotalBath\"] = train[\"BsmtFullBath\"] + (0.5 * train[\"BsmtHalfBath\"]) + \\\ntrain[\"FullBath\"] + (0.5 * train[\"HalfBath\"])\n# Total SF for house (incl. basement)\ntrain[\"AllSF\"] = train[\"GrLivArea\"] + train[\"TotalBsmtSF\"]\n# Total SF for 1st + 2nd floors\ntrain[\"AllFlrsSF\"] = train[\"1stFlrSF\"] + train[\"2ndFlrSF\"]\n# Total SF for porch\ntrain[\"AllPorchSF\"] = train[\"OpenPorchSF\"] + train[\"EnclosedPorch\"] + \\\ntrain[\"3SsnPorch\"] + train[\"ScreenPorch\"]\n# Has masonry veneer or not\ntrain[\"HasMasVnr\"] = train.MasVnrType.replace({\"BrkCmn\" : 1, \"BrkFace\" : 1, \"CBlock\" : 1, \n                                               \"Stone\" : 1, \"None\" : 0})\n# House completed before sale or not\ntrain[\"BoughtOffPlan\"] = train.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \n                                                      \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a345817d-8b0c-7467-7524-16a4cbb02794"},"outputs":[],"source":"# Find most important features relative to target\nprint(\"Find most important features relative to target\")\ncorr = train.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr.SalePrice)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf2a8386-cb11-969a-d5cb-a2bb4a7094fd"},"outputs":[],"source":"# Create new features\n# 3* Polynomials on the top 10 existing features\ntrain[\"OverallQual-s2\"] = train[\"OverallQual\"] ** 2\ntrain[\"OverallQual-s3\"] = train[\"OverallQual\"] ** 3\ntrain[\"OverallQual-Sq\"] = np.sqrt(train[\"OverallQual\"])\ntrain[\"AllSF-2\"] = train[\"AllSF\"] ** 2\ntrain[\"AllSF-3\"] = train[\"AllSF\"] ** 3\ntrain[\"AllSF-Sq\"] = np.sqrt(train[\"AllSF\"])\ntrain[\"AllFlrsSF-2\"] = train[\"AllFlrsSF\"] ** 2\ntrain[\"AllFlrsSF-3\"] = train[\"AllFlrsSF\"] ** 3\ntrain[\"AllFlrsSF-Sq\"] = np.sqrt(train[\"AllFlrsSF\"])\ntrain[\"GrLivArea-2\"] = train[\"GrLivArea\"] ** 2\ntrain[\"GrLivArea-3\"] = train[\"GrLivArea\"] ** 3\ntrain[\"GrLivArea-Sq\"] = np.sqrt(train[\"GrLivArea\"])\ntrain[\"SimplOverallQual-s2\"] = train[\"SimplOverallQual\"] ** 2\ntrain[\"SimplOverallQual-s3\"] = train[\"SimplOverallQual\"] ** 3\ntrain[\"SimplOverallQual-Sq\"] = np.sqrt(train[\"SimplOverallQual\"])\ntrain[\"ExterQual-2\"] = train[\"ExterQual\"] ** 2\ntrain[\"ExterQual-3\"] = train[\"ExterQual\"] ** 3\ntrain[\"ExterQual-Sq\"] = np.sqrt(train[\"ExterQual\"])\ntrain[\"GarageCars-2\"] = train[\"GarageCars\"] ** 2\ntrain[\"GarageCars-3\"] = train[\"GarageCars\"] ** 3\ntrain[\"GarageCars-Sq\"] = np.sqrt(train[\"GarageCars\"])\ntrain[\"TotalBath-2\"] = train[\"TotalBath\"] ** 2\ntrain[\"TotalBath-3\"] = train[\"TotalBath\"] ** 3\ntrain[\"TotalBath-Sq\"] = np.sqrt(train[\"TotalBath\"])\ntrain[\"KitchenQual-2\"] = train[\"KitchenQual\"] ** 2\ntrain[\"KitchenQual-3\"] = train[\"KitchenQual\"] ** 3\ntrain[\"KitchenQual-Sq\"] = np.sqrt(train[\"KitchenQual\"])\ntrain[\"GarageScore-2\"] = train[\"GarageScore\"] ** 2\ntrain[\"GarageScore-3\"] = train[\"GarageScore\"] ** 3\ntrain[\"GarageScore-Sq\"] = np.sqrt(train[\"GarageScore\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a8ce5ba-12b9-61d6-6ae8-d3ff838f082e"},"outputs":[],"source":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = train.select_dtypes(include = [\"object\"]).columns\nnumerical_features = train.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features = numerical_features.drop(\"SalePrice\")\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ntrain_num = train[numerical_features]\ntrain_cat = train[categorical_features]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7fed0c33-faa1-e455-c834-7ec12f6d33e5"},"outputs":[],"source":"# Handle remaining missing values for numerical features by using median as replacement\nprint(\"NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))\ntrain_num = train_num.fillna(train_num.median())\nprint(\"Remaining NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"571b9524-6559-d088-1917-9f290a662af1"},"outputs":[],"source":"# Log transform of the skewed numerical features to lessen impact of outliers\n# Inspired by Alexandru Papiu's script : https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models\n# As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed\nskewness = train_num.apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\nprint(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\nskewed_features = skewness.index\ntrain_num[skewed_features] = np.log1p(train_num[skewed_features])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93029443-cbb3-e2ee-9c4a-a61e22ef1a15"},"outputs":[],"source":"# Create dummy features for categorical values via one-hot encoding\nprint(\"NAs for categorical features in train : \" + str(train_cat.isnull().values.sum()))\ntrain_cat = pd.get_dummies(train_cat)\nprint(\"Remaining NAs for categorical features in train : \" + str(train_cat.isnull().values.sum()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"a1ae46d1-8787-67a3-2f69-6497c97320eb"},"source":"**Modeling**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3685f20a-c332-7ab0-cced-03d639d28833"},"outputs":[],"source":"# Join categorical and numerical features\ntrain = pd.concat([train_num, train_cat], axis = 1)\nprint(\"New number of features : \" + str(train.shape[1]))\n\n# Partition the dataset in train + validation sets\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.3, random_state = 0)\nprint(\"X_train : \" + str(X_train.shape))\nprint(\"X_test : \" + str(X_test.shape))\nprint(\"y_train : \" + str(y_train.shape))\nprint(\"y_test : \" + str(y_test.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1279ec22-61c2-4d5a-b8da-9dcadbda564d"},"outputs":[],"source":"# Standardize numerical features\nstdSc = StandardScaler()\nX_train.loc[:, numerical_features] = stdSc.fit_transform(X_train.loc[:, numerical_features])\nX_test.loc[:, numerical_features] = stdSc.transform(X_test.loc[:, numerical_features])"},{"cell_type":"markdown","metadata":{"_cell_guid":"ef95410a-0008-c898-56f4-bde9a683aba5"},"source":"Standardization cannot be done before  the partitioning, as we don't want to fit the StandardScaler on some observations that will later be used in the test set."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e06d07c0-b968-f28e-1c2e-389fbc48e9f8"},"outputs":[],"source":"# Define error measure for official scoring : RMSE\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)"},{"cell_type":"markdown","metadata":{"_cell_guid":"72acb8a1-5ee5-ce21-1620-5109e1742197"},"source":"Note : I'm not getting nearly the same numbers in local CV compared to public LB, so I'm a tad worried that my CV process may have an issue somewhere. If you spot something, please let me know."},{"cell_type":"markdown","metadata":{"_cell_guid":"9fe1c63e-3803-feac-362c-631399fdb8ec"},"source":"**1* Linear Regression without regularization**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"101cf15a-9006-ac9a-8abe-756371fd8b1a"},"outputs":[],"source":"# Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2ce473a8-1d21-76e6-1c28-99c2a5c59541"},"source":"RMSE on Training set shows up weird here (not when I run it on my computer) for some reason.  \nErrors seem randomly distributed and randomly scattered around the centerline, so there is that at least. It means our model was able to capture most of the explanatory information."},{"cell_type":"markdown","metadata":{"_cell_guid":"a70803d8-638c-bdfa-6897-aed8ce78f475"},"source":"**2* Linear Regression with Ridge regularization (L2 penalty)**\n\nFrom the *Python Machine Learning* book by Sebastian Raschka :  Regularization is a very useful method to handle collinearity, filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights.  \n\nRidge regression is an L2 penalized model where we simply add the squared sum of the weights to our cost function."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1fb3f0d6-a070-9ce5-7cc3-01a7e26faa4f"},"outputs":[],"source":"# 2* Ridge\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_rdg, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(ridge.coef_, index = X_train.columns)\nprint(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6e4193a5-7734-be2d-4ad7-460211cbd65e"},"source":"We're getting a much better RMSE result now that we've added regularization. The very small difference between training and test results indicate that we eliminated most of the overfitting. Visually, the graphs seem to confirm that idea.  \n\nRidge used almost all of the existing features."},{"cell_type":"markdown","metadata":{"_cell_guid":"2da33461-fb1a-c09a-b0fe-d20e8c8f52e3"},"source":"**3* Linear Regression with Lasso regularization (L1 penalty)**\n\nLASSO stands for *Least Absolute Shrinkage and Selection Operator*. It is an alternative regularization method, where we simply replace the square of the weights by the sum of the absolute value of the weights. In contrast to L2 regularization, L1 regularization yields sparse feature vectors : most feature weights will be zero. Sparsity can be useful in practice if we have a high dimensional dataset with many features that are irrelevant.  \n\nWe can suspect that it should be more efficient than Ridge here."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8525724a-fb77-66d4-e06f-f3365fbd8ec3"},"outputs":[],"source":"# 3* Lasso\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())\ny_train_las = lasso.predict(X_train)\ny_test_las = lasso.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_las, y_train_las - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test_las - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_las, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"5d979d30-a58b-cdb5-18cd-78842fa0e42e"},"source":"RMSE results are better both on training and test sets. The most interesting thing is that Lasso used only one third of the available features. Another interesting tidbit : it seems to give big weights to Neighborhood categories, both in positive and negative ways. Intuitively it makes sense, house prices change a whole lot from one neighborhood to another in the same city.  \n\nThe \"MSZoning_C (all)\" feature seems to have a disproportionate impact compared to the others. It is defined as *general zoning classification : commercial*. It seems a bit weird to me that having your house in a mostly commercial zone would be such a terrible thing."},{"cell_type":"markdown","metadata":{"_cell_guid":"80ffe575-2022-2802-9994-858c06786e68"},"source":"**4* Linear Regression with ElasticNet regularization (L1 and L2 penalty)**\n\nElasticNet is a compromise between Ridge and Lasso regression. It has a L1 penalty to generate sparsity and a L2 penalty to overcome some of the limitations of Lasso, such as the number of variables (Lasso can't select more features than it has observations, but it's not the case here anyway)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a641b34-5bdc-a183-c442-6cc91c506e86"},"outputs":[],"source":"# 4* ElasticNet\nelasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\nprint(\"ElasticNet RMSE on Test set :\", rmse_cv_test(elasticNet).mean())\ny_train_ela = elasticNet.predict(X_train)\ny_test_ela = elasticNet.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_ela, y_train_ela - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_ela, y_test_ela - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with ElasticNet regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train, y_train_ela, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test, y_test_ela, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with ElasticNet regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(elasticNet.coef_, index = X_train.columns)\nprint(\"ElasticNet picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ElasticNet Model\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"61b7db78-5628-75e9-016a-067004049ddc"},"source":"The optimal L1 ratio used by ElasticNet here is equal to 1, which means it is exactly equal to the Lasso regressor we used earlier (and had it been equal to 0, it would have been exactly equal to our Ridge regressor). The model didn't need any L2 regularization to overcome any potential L1 shortcoming."},{"cell_type":"markdown","metadata":{"_cell_guid":"a497362f-bad8-99ad-2320-d651435723d3"},"source":"Note : I tried to remove the \"MSZoning_C (all)\" feature, it resulted in a slightly worse CV score, but slightly better public LB score."},{"cell_type":"markdown","metadata":{"_cell_guid":"9bce5b62-c17b-d1cf-e3e6-2b88dbbc9d7f"},"source":"**Conclusion**  \n\nPutting time and effort into preparing the dataset and optimizing the regularization resulted in a decent score, better than some public scripts which use algorithms that historically perform better in Kaggle contests, like Random Forests. Being fairly new to the world of machine learning contests, I will appreciate any constructive pointer to improve, and I thank you for your time."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}