{"cells":[{"metadata":{"_uuid":"0563e5e513745ab126d543854f1871b58efc4eb6"},"cell_type":"markdown","source":"<H1><center>Predict future sales</center></H1>"},{"metadata":{"_uuid":"74e4a7fea8e6b478f09cc8fb90e1ef1bb508140f"},"cell_type":"markdown","source":"We are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.\n\nYou are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n\n\n\n### Data fields description:\n* ID - an Id that represents a (Shop, Item) tuple within the test set\n* shop_id - unique identifier of a shop\n* item_id - unique identifier of a product\n* item_category_id - unique identifier of item category\n* date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n* date - date in format dd/mm/yyyy\n* item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n* item_price - current price of an item\n* item_name - name of item\n* shop_name - name of shop\n* item_category_name - name of item category\n\n\n### Dependencies"},{"metadata":{"trusted":true,"_uuid":"60785124984e36422c50287be4d6b1d7944af345","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"import datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport catboost\nfrom catboost import Pool\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dc8d7422c27df8c88b6b8493c2967da432300a8"},"cell_type":"markdown","source":"### Loading data"},{"metadata":{"trusted":true,"_uuid":"1902600704b6188ec08cd65ae0df9b6541b02a6c"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv', dtype={'ID': 'int32', 'shop_id': 'int32', \n                                                  'item_id': 'int32'})\nitem_categories = pd.read_csv('../input/item_categories.csv', \n                              dtype={'item_category_name': 'str', 'item_category_id': 'int32'})\nitems = pd.read_csv('../input/items.csv', dtype={'item_name': 'str', 'item_id': 'int32', \n                                                 'item_category_id': 'int32'})\nshops = pd.read_csv('../input/shops.csv', dtype={'shop_name': 'str', 'shop_id': 'int32'})\nsales = pd.read_csv('../input/sales_train.csv', parse_dates=['date'], \n                    dtype={'date': 'str', 'date_block_num': 'int32', 'shop_id': 'int32', \n                          'item_id': 'int32', 'item_price': 'float32', 'item_cnt_day': 'int32'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da903a7d023c46a81b2e9297073cf6b1941febfc"},"cell_type":"markdown","source":"### Join data sets"},{"metadata":{"trusted":true,"_uuid":"203e9c8159e3f7b9369624eb070b15b255b64003"},"cell_type":"code","source":"train = sales.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(item_categories, on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15abfe223ffc7b1f70a373faaf49bccc6fdefc2d"},"cell_type":"markdown","source":"### Let's take a look at the raw data"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"88cdd03ee5d313904780fa4797b9eff9ae88bfcb"},"cell_type":"code","source":"print('Train rows: ', train.shape[0])\nprint('Train columns: ', train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46a43d3fe0fffcc8b8d36ffd3df9d3c291bcb098","_kg_hide-input":true},"cell_type":"code","source":"train.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae26b7e92a34b929d8054ae3e4a6908c50ced7fb","_kg_hide-input":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"038c612e2f3177010080cbc2d0bd07d2e6d81af0"},"cell_type":"markdown","source":"### Time period of the dataset"},{"metadata":{"trusted":true,"_uuid":"4943dc5bd0e533852c35a27fae90f7563c7b1b6c"},"cell_type":"code","source":"print('Min date from train set: %s' % train['date'].min().date())\nprint('Max date from train set: %s' % train['date'].max().date())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10ae6e2516acc7cf9d1fba866af8608af5f37c1d"},"cell_type":"markdown","source":"### Data leakages\n\nAbout data leakages I'll only be using only the \"shop_id\" and \"item_id\" that appear on the test set."},{"metadata":{"trusted":true,"_uuid":"54c95a8b06b213d92cff944e6364168f8a891682"},"cell_type":"code","source":"test_shop_ids = test['shop_id'].unique()\ntest_item_ids = test['item_id'].unique()\n# Only shops that exist in test set.\nlk_train = train[train['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\nlk_train = lk_train[lk_train['item_id'].isin(test_item_ids)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3fafd1a419a27ac623be0d5dcb6a3109176472a"},"cell_type":"code","source":"print('Data set size before leaking:', train.shape[0])\nprint('Data set size after leaking:', lk_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e293477b6db49ceec812a68536b282948f1463a"},"cell_type":"markdown","source":"### Data cleaning\n\n    Only records with \"item_price\" > 0."},{"metadata":{"trusted":true,"_uuid":"cefc1f8d4328fb583941704a9423eb0aaf051936"},"cell_type":"code","source":"train = train.query('item_price > 0')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc7d2781916d506fafc6dedda43c981d05a4c48d"},"cell_type":"markdown","source":"### Data preprocessing\n* I'm dropping the text features since I won't be doing anything with them.\n* We are asked to predict total sales for every product and store in the next month, and our data is given by day, so let's remove unwanted columns and aggregate the data by month."},{"metadata":{"trusted":true,"_uuid":"de93738c724479d8234e30180ebcd8bf6becaf43"},"cell_type":"code","source":"# Select only useful features.\ntrain_monthly = lk_train[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cfa016f89affd460f056a8107cfc8f345773907"},"cell_type":"code","source":"# Group by month in this case \"date_block_num\" and aggregate features.\ntrain_monthly = train_monthly.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False)\ntrain_monthly = train_monthly.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n# Rename features.\ntrain_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6790d154fe62a879dd23ed5c7f36704eeb4fe600"},"cell_type":"markdown","source":"#### To mimic the real behavior of the data we have to create the missing records from the loaded dataset, so for each month we need to create the missing records for each shop and item, since we don't have data for them I'll replace them with 0."},{"metadata":{"trusted":true,"_uuid":"0491342541802ddef275456aeba8a25a152bd1be"},"cell_type":"code","source":"# Build a data set with all the possible combinations of ['date_block_num','shop_id','item_id'] so we won't have missing records.\nshop_ids = train_monthly['shop_id'].unique()\nitem_ids = train_monthly['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fbb090d2d8642622f01c6c52e362f74fa0574ce"},"cell_type":"code","source":"# Merge the train set with the complete set (missing records will be filled with 0).\ntrain_monthly = pd.merge(empty_df, train_monthly, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_monthly.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c59d6b48a35e04a80c13ff298e704a47c62fab2"},"cell_type":"markdown","source":"### New dataset"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"53c97644dde67fc5284c7fd658752f54e55b3adb"},"cell_type":"code","source":"train_monthly.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"848d24b66d9b499e0f7a369b32ca2adfa64ce55d"},"cell_type":"code","source":"train_monthly.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f31f80982ae23b65c6cd3b78ac6d72613d79db69"},"cell_type":"code","source":"# Extract time based features.\ntrain_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x//12) + 2013))\ntrain_monthly['month'] = train_monthly['date_block_num'].apply(lambda x: (x % 12))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47e69a110e7cbbda02a5335284b3cf7840bad1e5"},"cell_type":"markdown","source":"### EDA"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1d5a8d6bb8c546dfd59cb323a3aefb2e21f089e3"},"cell_type":"code","source":"# Grouping data for EDA.\ngp_month_mean = train_monthly.groupby(['month'], as_index=False)['item_cnt'].mean()\ngp_month_sum = train_monthly.groupby(['month'], as_index=False)['item_cnt'].sum()\ngp_category_mean = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].mean()\ngp_category_sum = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].sum()\ngp_shop_mean = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].mean()\ngp_shop_sum = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da01e74039e1387b6cdf4a1ace8ac9c8a1fe2f3a"},"cell_type":"markdown","source":"### How sales behaves along the year?"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d17df33e559832a26514aa0598578cf4d780a783"},"cell_type":"code","source":"f, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\nsns.lineplot(x=\"month\", y=\"item_cnt\", data=gp_month_mean, ax=axes[0]).set_title(\"Monthly mean\")\nsns.lineplot(x=\"month\", y=\"item_cnt\", data=gp_month_sum, ax=axes[1]).set_title(\"Monthly sum\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62d85d98651690f0f9905bbbee1cef1cb445467d"},"cell_type":"markdown","source":"As we can see we have a trending increase of item sales count (mean) towards the ending of the year."},{"metadata":{"_uuid":"edc8f8451e7ffaa4f3da558749e871e2b2ef4a53"},"cell_type":"markdown","source":"### What category sells more?"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"bc17569ef37ca05d8095ed3c2497fad1d12fc7d2"},"cell_type":"code","source":"f, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\nsns.barplot(x=\"item_category_id\", y=\"item_cnt\", data=gp_category_mean, ax=axes[0], palette=\"rocket\").set_title(\"Monthly mean\")\nsns.barplot(x=\"item_category_id\", y=\"item_cnt\", data=gp_category_sum, ax=axes[1], palette=\"rocket\").set_title(\"Monthly sum\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c66c0208451f565f8bcc889aaaf2ef63a663e310"},"cell_type":"markdown","source":"Also only few of the categories seems to hold most of the sell count."},{"metadata":{"_uuid":"f40b9c8d09d4406657d6a5fd632f6faf80560934"},"cell_type":"markdown","source":"### What shop sells more?"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e3697d0b3f72ffd76c4e980fc0e4eda7c56e551a"},"cell_type":"code","source":"f, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\nsns.barplot(x=\"shop_id\", y=\"item_cnt\", data=gp_shop_mean, ax=axes[0], palette=\"rocket\").set_title(\"Monthly mean\")\nsns.barplot(x=\"shop_id\", y=\"item_cnt\", data=gp_shop_sum, ax=axes[1], palette=\"rocket\").set_title(\"Monthly sum\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27f8b2a14c4a629fbf8d00378df36c61f0dab6f7"},"cell_type":"markdown","source":"Most of the shops have a similar sell rate, but 3 of them have a much higher rate, this may be a indicative of the shop size."},{"metadata":{"_uuid":"b0197e0bd9898b0a7c3861636708bda74311949f"},"cell_type":"markdown","source":"### Checking for outliers"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7ec1f1c8a32f994519982d975bf81120e0862adc"},"cell_type":"code","source":"sns.jointplot(x=\"item_cnt\", y=\"item_price\", data=train_monthly, height=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a195281a43adca4c46eb802e49ee964468da8e95"},"cell_type":"code","source":"sns.jointplot(x=\"item_cnt\", y=\"transactions\", data=train_monthly, height=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c68744022333cb0b0aac42c3f173eca4a40c1d2"},"cell_type":"markdown","source":"### Feature \"item_cnt\" distribution."},{"metadata":{"trusted":true,"_uuid":"22dc755fb7984aa016def26c104a1a7b24c2ed08","_kg_hide-input":true},"cell_type":"code","source":"plt.subplots(figsize=(22, 8))\nsns.boxplot(train_monthly['item_cnt'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1c36960fcfc7ba663d2d7b95af4a1d5b5d0326b"},"cell_type":"markdown","source":"#### Removing outliers\nI'll treat \"item_cnt\" > 20 and < 0, \"item_price\" >= 400000 as outliers, so I'll remove them."},{"metadata":{"trusted":true,"_uuid":"5d4f1ba256e10cca24bbf1f62fc504809cf40871"},"cell_type":"code","source":"train_monthly = train_monthly.query('item_cnt >= 0 and item_cnt <= 20 and item_price < 400000')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84c6a22589ec31a2283140c23a747c179d6694a5"},"cell_type":"markdown","source":"### Creating the label\nOur label will be the \"item_cnt\" of the next month, as we are dealing with a forecast problem."},{"metadata":{"trusted":true,"_uuid":"e790012087340e3fbf72c10ffa953255528d1a52"},"cell_type":"code","source":"train_monthly['item_cnt_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Feature engineering\n\n#### Unitary item prices."},{"metadata":{"trusted":true,"_uuid":"b49cf1f316bd467149fe948482210cc474edf8ff"},"cell_type":"code","source":"train_monthly['item_price_unit'] = train_monthly['item_price'] // train_monthly['item_cnt']\ntrain_monthly['item_price_unit'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc85cb53741fb9efe03de4573f15674f7e01868a"},"cell_type":"markdown","source":"#### Group based features."},{"metadata":{"trusted":true,"_uuid":"f4d3a338b82b6a0a4f00f8c46af98d43c41bf6cf"},"cell_type":"code","source":"gp_item_price = train_monthly.sort_values('date_block_num').groupby(['item_id'], as_index=False).agg({'item_price':[np.min, np.max]})\ngp_item_price.columns = ['item_id', 'hist_min_item_price', 'hist_max_item_price']\n\ntrain_monthly = pd.merge(train_monthly, gp_item_price, on='item_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0450d445b6d973b3716967d761f884990991afad"},"cell_type":"markdown","source":"#### How much each item's price changed from its (lowest/highest) historical price."},{"metadata":{"trusted":true,"_uuid":"b884111dcc00cf448f08f989ae6952bfd86af9ac"},"cell_type":"code","source":"train_monthly['price_increase'] = train_monthly['item_price'] - train_monthly['hist_min_item_price']\ntrain_monthly['price_decrease'] = train_monthly['hist_max_item_price'] - train_monthly['item_price']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e90df0bcb5588dbf252f2aaf2b3135963d187f85"},"cell_type":"markdown","source":"#### Rolling window based features (window = 3 months)."},{"metadata":{"trusted":true,"_uuid":"c1763430ebcdade655aad80f16cc114d6b64c5fb"},"cell_type":"code","source":"# Min value\nf_min = lambda x: x.rolling(window=3, min_periods=1).min()\n# Max value\nf_max = lambda x: x.rolling(window=3, min_periods=1).max()\n# Mean value\nf_mean = lambda x: x.rolling(window=3, min_periods=1).mean()\n# Standard deviation\nf_std = lambda x: x.rolling(window=3, min_periods=1).std()\n\nfunction_list = [f_min, f_max, f_mean, f_std]\nfunction_name = ['min', 'max', 'mean', 'std']\n\nfor i in range(len(function_list)):\n    train_monthly[('item_cnt_%s' % function_name[i])] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].apply(function_list[i])\n\n# Fill the empty std features with 0\ntrain_monthly['item_cnt_std'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14fd3a220a05de5563597c7d2efd21309e03dd34"},"cell_type":"markdown","source":"#### Lag based features."},{"metadata":{"trusted":true,"_uuid":"8660a0955ffc6f51d1769a30606ffca78792dc7e"},"cell_type":"code","source":"lag_list = [1, 2, 3]\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly[ft_name] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n    # Fill the empty shifted features with 0\n    train_monthly[ft_name].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67d5a74531a16b50093d49c9093aa918854664ed"},"cell_type":"markdown","source":"#### Item sales count trend."},{"metadata":{"trusted":true,"_uuid":"0217e8b8ec14a43f733d241d2104ad411b965944"},"cell_type":"code","source":"train_monthly['item_trend'] = train_monthly['item_cnt']\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly['item_trend'] -= train_monthly[ft_name]\n\ntrain_monthly['item_trend'] /= len(lag_list) + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a440f63fc8e5e203f6a421d7863f39a85da75fc9"},"cell_type":"markdown","source":"### Dataset after feature engineering"},{"metadata":{"trusted":true,"_uuid":"f86a1a03e9df91f0ae526e7f1b3794a73e605657","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"train_monthly.head().T","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f1c265db6d6363b18c684002edeed7fae6407b9b"},"cell_type":"code","source":"train_monthly.describe().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b896cee032d77260bc2cc41189f1ab0c5b272f1f"},"cell_type":"markdown","source":"### Train/validation split\n* As we know the test set in on the future, so we should try to simulate the same distribution on our train/validation split.\n* Our train set will be the first 3~28 blocks, validation will be last 5 blocks (29~32) and test will be block 33.\n* I'm leaving the first 3 months out because we use a 3 month window to generate features, so these first 3 month won't have really windowed useful features."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"02d50cae77983376643ecabb9e477220a94d28b2"},"cell_type":"code","source":"train_set = train_monthly.query('date_block_num >= 3 and date_block_num < 28').copy()\nvalidation_set = train_monthly.query('date_block_num >= 28 and date_block_num < 33').copy()\ntest_set = train_monthly.query('date_block_num == 33').copy()\n\ntrain_set.dropna(subset=['item_cnt_month'], inplace=True)\nvalidation_set.dropna(subset=['item_cnt_month'], inplace=True)\n\ntrain_set.dropna(inplace=True)\nvalidation_set.dropna(inplace=True)\n\nprint('Train set records:', train_set.shape[0])\nprint('Validation set records:', validation_set.shape[0])\nprint('Test set records:', test_set.shape[0])\n\nprint('Train set records: %s (%.f%% of complete data)' % (train_set.shape[0], ((train_set.shape[0]/train_monthly.shape[0])*100)))\nprint('Validation set records: %s (%.f%% of complete data)' % (validation_set.shape[0], ((validation_set.shape[0]/train_monthly.shape[0])*100)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60a594cf5659f2d352efdc449a9124d9ad682537"},"cell_type":"markdown","source":"### Mean encoding.\n* done after the train/validation split."},{"metadata":{"trusted":true,"_uuid":"c2865c55c01679d9c8a4a695e0af198a1800db33"},"cell_type":"code","source":"# Shop mean encoding.\ngp_shop_mean = train_set.groupby(['shop_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_mean.columns = ['shop_mean']\ngp_shop_mean.reset_index(inplace=True)\n# Item mean encoding.\ngp_item_mean = train_set.groupby(['item_id']).agg({'item_cnt_month': ['mean']})\ngp_item_mean.columns = ['item_mean']\ngp_item_mean.reset_index(inplace=True)\n# Shop with item mean encoding.\ngp_shop_item_mean = train_set.groupby(['shop_id', 'item_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_item_mean.columns = ['shop_item_mean']\ngp_shop_item_mean.reset_index(inplace=True)\n# Year mean encoding.\ngp_year_mean = train_set.groupby(['year']).agg({'item_cnt_month': ['mean']})\ngp_year_mean.columns = ['year_mean']\ngp_year_mean.reset_index(inplace=True)\n# Month mean encoding.\ngp_month_mean = train_set.groupby(['month']).agg({'item_cnt_month': ['mean']})\ngp_month_mean.columns = ['month_mean']\ngp_month_mean.reset_index(inplace=True)\n\n# Add meand encoding features to train set.\ntrain_set = pd.merge(train_set, gp_shop_mean, on=['shop_id'], how='left')\ntrain_set = pd.merge(train_set, gp_item_mean, on=['item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_year_mean, on=['year'], how='left')\ntrain_set = pd.merge(train_set, gp_month_mean, on=['month'], how='left')\n# Add meand encoding features to validation set.\nvalidation_set = pd.merge(validation_set, gp_shop_mean, on=['shop_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_item_mean, on=['item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_year_mean, on=['year'], how='left')\nvalidation_set = pd.merge(validation_set, gp_month_mean, on=['month'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b3118d00fcb8fc2c1030a5ff37d8c201b989545"},"cell_type":"code","source":"# Create train and validation sets and labels. \nX_train = train_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_train = train_set['item_cnt_month'].astype(int)\nX_validation = validation_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_validation = validation_set['item_cnt_month'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10a282455b0ba0c601eea05bc90322877a132429"},"cell_type":"code","source":"# Integer features (used by catboost model).\nint_features = ['shop_id', 'item_id', 'year', 'month']\n\nX_train[int_features] = X_train[int_features].astype('int32')\nX_validation[int_features] = X_validation[int_features].astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c61ed42f58832da6b058b52f27bb329a6b1508a9"},"cell_type":"markdown","source":"### Build test set\nWe want to predict for \"date_block_num\" 34 so our test set will be block 33 and our predictions should reflect block 34 values. In other words we use block 33 because we want to forecast values for block 34."},{"metadata":{"trusted":true,"_uuid":"ea92bf4c0739cb94e6b8b06b6525cab66a47fa6b"},"cell_type":"code","source":"latest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nX_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nX_test['year'] = 2015\nX_test['month'] = 9\nX_test.drop('item_cnt_month', axis=1, inplace=True)\nX_test[int_features] = X_test[int_features].astype('int32')\nX_test = X_test[X_train.columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7263b8e69d7f1fe529b89c08a1cb905ed4bbf257"},"cell_type":"markdown","source":"### Replacing missing values."},{"metadata":{"trusted":true,"_uuid":"7751f418cefd6a4daf7663ec43b4baadaa3b3905","_kg_hide-input":false},"cell_type":"code","source":"sets = [X_train, X_validation, X_test]\n\n# This was taking too long.\n# Replace missing values with the median of each item.\n# for dataset in sets:\n#     for item_id in dataset['item_id'].unique():\n#         for column in dataset.columns:\n#             item_median = dataset[(dataset['item_id'] == item_id)][column].median()\n#             dataset.loc[(dataset[column].isnull()) & (dataset['item_id'] == item_id), column] = item_median\n\n# Replace missing values with the median of each shop.            \nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n            \n# Fill remaining missing values on test set with mean.\nX_test.fillna(X_test.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0956e6c1ef3bbb3908d054b7a86b596404e5aa8b"},"cell_type":"code","source":"# I'm dropping \"item_category_id\", we don't have it on test set and would be a little hard to create categories for items that exist only on test set.\nX_train.drop(['item_category_id'], axis=1, inplace=True)\nX_validation.drop(['item_category_id'], axis=1, inplace=True)\nX_test.drop(['item_category_id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4179d24399b913be389cd059863dff6fdade5cc"},"cell_type":"markdown","source":"### Test set"},{"metadata":{"trusted":true,"_uuid":"7cc6b59eb68c97e706c505beb84c1fe291373556","_kg_hide-input":true},"cell_type":"code","source":"X_test.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fdbbe2bedefb57812e98777717026b1f89ad87f","_kg_hide-input":true},"cell_type":"code","source":"X_test.describe().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff43ed998eb859284670c8500ff854cde5303f9c"},"cell_type":"markdown","source":"### Modeling the data\n\n## Tree based models\n\n### Catboost"},{"metadata":{"trusted":true,"_uuid":"73f88a81d81cdabf0da83e59015cd0edfc2ed6f8","scrolled":true},"cell_type":"code","source":"cat_features = [0, 1, 7, 8]\n\ncatboost_model = CatBoostRegressor(\n    iterations=500,\n    max_ctr_complexity=4,\n    random_seed=0,\n    od_type='Iter',\n    od_wait=25,\n    verbose=50,\n    depth=4\n)\n\ncatboost_model.fit(\n    X_train, Y_train,\n    cat_features=cat_features,\n    eval_set=(X_validation, Y_validation)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03a0b86ba70914bae6f48d00756b1fb987fd003e","_kg_hide-input":true},"cell_type":"code","source":"print('Model params:', catboost_model.get_params())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca296eb489cf8b4d47352bf2ffc0fcf464753abe"},"cell_type":"markdown","source":"### Catboost feature importance"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3f0aad55bd1b8f2381c43ab36775d5e7eb21a447"},"cell_type":"code","source":"feature_score = pd.DataFrame(list(zip(X_train.dtypes.index, catboost_model.get_feature_importance(Pool(X_train, label=Y_train, cat_features=cat_features)))), columns=['Feature','Score'])\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n\nplt.rcParams[\"figure.figsize\"] = (19, 6)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\nax.set_xlabel('')\nrects = ax.patches\nlabels = feature_score['Score'].round(2)\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 0.35, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbea78e85b720043785c6022ad2cc1f67ca08609","_kg_hide-input":false},"cell_type":"code","source":"catboost_train_pred = catboost_model.predict(X_train)\ncatboost_val_pred = catboost_model.predict(X_validation)\ncatboost_test_pred = catboost_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"725c3e57aadc1de507b25293117cc7b31c0ff488"},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, catboost_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, catboost_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1098b04b6a111e07c7906ccb50520f1f40a47f54"},"cell_type":"markdown","source":"### Let's see how catboost performed with this \"prediction x label\" plot.\nThe closer the points are to the middle dashed line the better are the predictions."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e246d057fdf5bf935fbd0c178dd492a856c0486c"},"cell_type":"code","source":"def model_performance_sc_plot(predictions, labels, title):\n    # Get min and max values of the predictions and labels.\n    min_val = max(max(predictions), max(labels))\n    max_val = min(min(predictions), min(labels))\n    # Create dataframe with predicitons and labels.\n    performance_df = pd.DataFrame({\"Label\":labels})\n    performance_df[\"Prediction\"] = predictions\n    # Plot data\n    sns.jointplot(y=\"Label\", x=\"Prediction\", data=performance_df, kind=\"reg\", height=7)\n    plt.plot([min_val, max_val], [min_val, max_val], 'm--')\n    plt.title(title, fontsize=9)\n    plt.show()\n    \n# model_performance_sc_plot(catboost_train_pred, Y_train, 'Train')\nmodel_performance_sc_plot(catboost_val_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"429b9cf8bb4b04dc8a103728fcafbbf5c886fcc0"},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true,"_uuid":"e2420e802e8d9bb292369da9fd3c30236db8ea41"},"cell_type":"code","source":"# Use only part of features on XGBoost.\nxgb_features = ['item_cnt','item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n                'item_cnt_shifted2', 'item_cnt_shifted3', 'shop_mean', \n                'shop_item_mean', 'item_trend', 'mean_item_cnt']\nxgb_train = X_train[xgb_features]\nxgb_val = X_validation[xgb_features]\nxgb_test = X_test[xgb_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"664e284578f124539a7fd444f4942c6938cad9a3"},"cell_type":"code","source":"xgb_model = XGBRegressor(max_depth=8, \n                         n_estimators=500, \n                         min_child_weight=1000,  \n                         colsample_bytree=0.7, \n                         subsample=0.7, \n                         eta=0.3, \n                         seed=0)\nxgb_model.fit(xgb_train, \n              Y_train, \n              eval_metric=\"rmse\", \n              eval_set=[(xgb_train, Y_train), (xgb_val, Y_validation)], \n              verbose=20, \n              early_stopping_rounds=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74aa82ad9e1007dca8991897576bbb9966e0de26"},"cell_type":"markdown","source":"### XGBoost feature importance"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"13818b135fd421ceb947cefab59d9f446855df50"},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (15, 6)\nplot_importance(xgb_model)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"187a10ac7bb5eb1053fd5649626d66835af65d20"},"cell_type":"code","source":"xgb_train_pred = xgb_model.predict(xgb_train)\nxgb_val_pred = xgb_model.predict(xgb_val)\nxgb_test_pred = xgb_model.predict(xgb_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"fe7b0f331a7099a0db15a90daaa58f2e89dd7ebf"},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, xgb_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, xgb_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08009dfa2b7ffdcaaf161d4bf22c44265e8cf0e3"},"cell_type":"markdown","source":"### Let's see how XGBoosting performed with this \"prediction x label\" plot."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4883eed3db7c700bed74fbd08a48b4fe448fa759"},"cell_type":"code","source":"# model_performance_sc_plot(xgb_train_pred, Y_train, 'Train')\nmodel_performance_sc_plot(xgb_val_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47a1a63f7fc895f20994975054a6c3d5c7c19759"},"cell_type":"markdown","source":"### Random forest"},{"metadata":{"trusted":true,"_uuid":"d2adddbafbb172a9c4e3e504e862f6df59a1fd93"},"cell_type":"code","source":"# Use only part of features on random forest.\nrf_features = ['shop_id', 'item_id', 'item_cnt', 'transactions', 'year',\n               'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n               'shop_mean', 'item_mean', 'item_trend', 'mean_item_cnt']\nrf_train = X_train[rf_features]\nrf_val = X_validation[rf_features]\nrf_test = X_test[rf_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb01123397fbd2cfd4ac3e0eefac456063b38810"},"cell_type":"code","source":"rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\nrf_model.fit(rf_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"1f58257f9673ec2221ca500bf7677e81cc0337ec"},"cell_type":"code","source":"rf_train_pred = rf_model.predict(rf_train)\nrf_val_pred = rf_model.predict(rf_val)\nrf_test_pred = rf_model.predict(rf_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f3962fc67733732b2ce1ddfc78ee528c8c76f63f"},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, rf_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, rf_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0ee13b99ee51d57d58bb34936ddbee90e36b86f"},"cell_type":"markdown","source":"### Let's see how random forest performed with this \"prediction x label\" plot."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c67466fda631c9ec6764cd453bd571dd290255cb"},"cell_type":"code","source":"# model_performance_sc_plot(rf_train_pred, Y_train, 'Train')\nmodel_performance_sc_plot(rf_val_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd12c4d9fc1957ba004cf25b49a17544f6d16a55"},"cell_type":"markdown","source":"## Linear models\n\n### Linear Regression"},{"metadata":{"trusted":true,"_uuid":"5ab33ceacdbec93d83e8d943703eaae913fc903b"},"cell_type":"code","source":"# Use only part of features on linear Regression.\nlr_features = ['item_cnt', 'item_cnt_shifted1', 'item_trend', 'mean_item_cnt', 'shop_mean']\nlr_train = X_train[lr_features]\nlr_val = X_validation[lr_features]\nlr_test = X_test[lr_features]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3b9c9a48b0f43d91e157ec8b74d2df76d8ea147"},"cell_type":"markdown","source":"#### Normalizing features"},{"metadata":{"trusted":true,"_uuid":"740e5d29b5b95efc5dccfd23583f1bf619511baf"},"cell_type":"code","source":"lr_scaler = MinMaxScaler()\nlr_scaler.fit(lr_train)\nlr_train = lr_scaler.transform(lr_train)\nlr_val = lr_scaler.transform(lr_val)\nlr_test = lr_scaler.transform(lr_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76fdad787667e8cb2390037ee4de0b797edd6b71"},"cell_type":"code","source":"lr_model = LinearRegression(n_jobs=-1)\nlr_model.fit(lr_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7d590deb7cce90bda5b917ceb5c51837010c7e6"},"cell_type":"code","source":"lr_train_pred = lr_model.predict(lr_train)\nlr_val_pred = lr_model.predict(lr_val)\nlr_test_pred = lr_model.predict(lr_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20a41af682fab380653753f1599b409eba76b1d7","_kg_hide-input":true},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, lr_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, lr_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f15a915bb673c8529d45e8a1a8561b78f353f068"},"cell_type":"markdown","source":"### Let's see how linear regression performed with this \"prediction x label\" plot."},{"metadata":{"trusted":true,"_uuid":"b0f4d203fb3024af2de7f6c0f9c8daec7f519ee9","_kg_hide-input":true},"cell_type":"code","source":"# model_performance_sc_plot(lr_train_pred, Y_train, 'Train')\nmodel_performance_sc_plot(lr_val_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"199d781d9fe50bdee22d9a0f7e015c478ad304a1"},"cell_type":"markdown","source":"## Clustering models\n\n### KNN Regressor"},{"metadata":{"trusted":true,"_uuid":"eacbbf35dcf870dd40971313f6679fca47be779e"},"cell_type":"code","source":"# Use only part of features on KNN.\nknn_features = ['item_cnt', 'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1',\n                'item_cnt_shifted2', 'shop_mean', 'shop_item_mean', \n                'item_trend', 'mean_item_cnt']\n\n# Subsample train set (using the whole data was taking too long).\nX_train_sampled = X_train[:100000]\nY_train_sampled = Y_train[:100000]\n\nknn_train = X_train_sampled[knn_features]\nknn_val = X_validation[knn_features]\nknn_test = X_test[knn_features]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1433659573f24e38c8cf9458a9a9776d65af9889"},"cell_type":"markdown","source":"#### Normalizing features"},{"metadata":{"trusted":true,"_uuid":"ceaccd5a99ffea219b457161b9969a92e2246dcf"},"cell_type":"code","source":"knn_scaler = MinMaxScaler()\nknn_scaler.fit(knn_train)\nknn_train = knn_scaler.transform(knn_train)\nknn_val = knn_scaler.transform(knn_val)\nknn_test = knn_scaler.transform(knn_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fab3dd11e5f5524105d3fc10833c54bff4337f8"},"cell_type":"code","source":"knn_model = KNeighborsRegressor(n_neighbors=9, leaf_size=13, n_jobs=-1)\nknn_model.fit(knn_train, Y_train_sampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1124addcec6c40076ff7056f3f29b7749761d52e"},"cell_type":"code","source":"knn_train_pred = knn_model.predict(knn_train)\nknn_val_pred = knn_model.predict(knn_val)\nknn_test_pred = knn_model.predict(knn_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a658907c91ee956e1da7feb3c92d81664720b5bd"},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train_sampled, knn_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, knn_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adf2895497d36eb4afe59df769510e9e265c2b9e"},"cell_type":"markdown","source":"### Let's see how knn performed with this \"prediction x label\" plot."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e2b76e52edbd2af4fe2b53e7d9851bfac24c3c69"},"cell_type":"code","source":"# model_performance_sc_plot(knn_train_pred, Y_train_sampled, 'Train')\nmodel_performance_sc_plot(knn_val_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b62f362b68c250eaa9aa6ba406c282ff577f9648"},"cell_type":"markdown","source":"### Create new datasets with the predictions from first level models.\n* Here I'll be using a simple ensembling technique, I'll use the 1st level models predictions as the input for the 2nd level model, this way the 2nd level model will basically use the 1st level models predictions as features and learn where to give more weight.\n* To use this technique I also need to use the 1st level models and make predictions on the test set, so I can use them on the 2nd level model.\n* I could also pass the complete validation set with extra features (the 1st level models prediction) to the 2nd level model and let it do a little more work on finding the solution."},{"metadata":{"trusted":true,"_uuid":"47525c6c9da7c560b06b037e02b932ef9c224333"},"cell_type":"code","source":"# Dataset that will be the train set of the ensemble model.\nfirst_level = pd.DataFrame(catboost_val_pred, columns=['catboost'])\nfirst_level['xgbm'] = xgb_val_pred\nfirst_level['random_forest'] = rf_val_pred\nfirst_level['linear_regression'] = lr_val_pred\nfirst_level['knn'] = knn_val_pred\nfirst_level['label'] = Y_validation.values\nfirst_level.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1457c21773856c8a7c73a452beb659f35464512"},"cell_type":"code","source":"# Dataset that will be the test set of the ensemble model.\nfirst_level_test = pd.DataFrame(catboost_test_pred, columns=['catboost'])\nfirst_level_test['xgbm'] = xgb_test_pred\nfirst_level_test['random_forest'] = rf_test_pred\nfirst_level_test['linear_regression'] = lr_test_pred\nfirst_level_test['knn'] = knn_test_pred\nfirst_level_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80a18fc9e973d3afdbd629b5735c7686e6252831"},"cell_type":"markdown","source":"### Ensembling\n\n* To combine the 1st level model predictions, I'll use a simple linear regression.\n* As I'm only feeding the model with predictions I don't need a complex model.\n\n#### Ensemble architecture:\n* 1st level:\n    * Catboost\n    * XGBM\n    * Random forest\n    * Linear Regression\n    * KNN\n* 2nd level;\n    * Linear Regression\n    \n#### Here is an  image to help the understanding\n \n <img src=\"https://raw.githubusercontent.com/dimitreOliveira/MachineLearning/master/Kaggle/Predict%20Future%20Sales/Ensemble%20Kaggle.jpg\" width=\"400\">"},{"metadata":{"_uuid":"8dbe7b6815d2ca1ce5f4f9861d7cfb1d2b7f7bf4"},"cell_type":"markdown","source":"### 2nd level model as a linear regression\n* This is the model that will combine the other ones to hopefully make an overall better prediction.\n* If the inputs to this mode were more complex, could be a good idea to split the data into train and validation again, this way you can check if the metal model is overfitting."},{"metadata":{"trusted":true,"_uuid":"e643b5f4b5c5ea9a1ee4ede6f26eabc710248feb"},"cell_type":"code","source":"meta_model = LinearRegression(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa8aa67fc031eeadbbd6df1854d81badda701116"},"cell_type":"markdown","source":"#### Trained on validation set using the 1st level models predictions as features."},{"metadata":{"trusted":true,"_uuid":"fa34fadee5290ef57b9998a345ca99a390030435"},"cell_type":"code","source":"# Drop label from dataset.\nfirst_level.drop('label', axis=1, inplace=True)\nmeta_model.fit(first_level, Y_validation)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab80de72ecaaa6db2db0ddd161a0715177e50c65"},"cell_type":"markdown","source":"#### Make predictions on test set using the 1st level models predictions as features."},{"metadata":{"trusted":true,"_uuid":"4e92b8cf8df4db809425ceb69c7cd01d457950c1","_kg_hide-input":true},"cell_type":"code","source":"ensemble_pred = meta_model.predict(first_level)\nfinal_predictions = meta_model.predict(first_level_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb5946b79a15317bb296f76a8b2df1037dd117f8"},"cell_type":"markdown","source":"#### Ensemble model metrics on validation set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e57727d067f5c5dc7f83589b8e4e0056f29d7815"},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(ensemble_pred, Y_validation)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d52b98625226102269934bf94f3602b0f036e9b8"},"cell_type":"markdown","source":"#### Let's see how the meta model performed with this \"prediction x label\" plot."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9d50eb5dc41518816096edf8789c14c571c24de1"},"cell_type":"code","source":"model_performance_sc_plot(ensemble_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab0063edbbdc9ff08d5f0bf207b9921380adeb98"},"cell_type":"markdown","source":"#### Output dataframe."},{"metadata":{"trusted":true,"_uuid":"d56956add596808733fe0474f93b3092ef1260f5"},"cell_type":"code","source":"prediction_df = pd.DataFrame(test['ID'], columns=['ID'])\nprediction_df['item_cnt_month'] = final_predictions.clip(0., 20.)\nprediction_df.to_csv('submission.csv', index=False)\nprediction_df.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}